# https://github.com/shicai/MobileNet-Caffe/issues/9
#
# net: "train_val.prototxt"
# #test_initialization: false
# #test_iter: 100
# #test_interval: 5000
# display: 20
# average_loss: 20
# base_lr: 0.1
# lr_policy: "poly"
# power: 1.0
# max_iter: 500000
# momentum: 0.9
# weight_decay: 0.0001
# snapshot: 5000
# snapshot_prefix: "mobilenet"

#  python compress_classifier.py -a mobilenet_050 --compress ../mobilenet/mobilenet_imagenet_baseline_training.yaml -j 22 ../../../data.imagenet -p 50 -b 256 --epochs 120 --lr 0.1 --wd 0.0001 --momentum 0.9
lr_schedulers:
  training_lr:
    class: PolynomialLR
    T_max: 120
    power: 1.0

policies:
    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1

# MobileNet V1
# https://arxiv.org/pdf/1704.04861.pdf
# MobileNet models were trained in TensorFlow [1] using RMSprop [33] with asynchronous gradient descent similar
# to Inception V3 [31]. However, contrary to training large models we use less regularization and data augmentation
# techniques because small models have less trouble with overfitting. When training MobileNets we do not use
# side heads or label smoothing and additionally reduce the amount image of distortions by limiting the size of small
# crops that are used in large Inception training [31]. Additionally, we found that it was important to put very little
# or no weight decay (l2 regularization) on the depthwise filters since their are so few parameters in them.

# Inception V3
# https://arxiv.org/pdf/1512.00567.pdf
# ... Our best models were achieved using RMSProp [21] with decay of 0.9 and eps = 1.0.
# We used a learning rate of 0.045, decayed every two epoch using an exponential rate of 0.94.

# python compress_classifier.py -a mobilenet_050 --compress ../mobilenet/mobilenet_imagenet_baseline_training.yaml -j 11 ~/datasets/imagenet -p 400 -b 32 --epochs 100 --lr 0.045 --wd 0.0001 --momentum 0 --gpu 0

#lr_schedulers:
#  training_lr:
#    class: ExponentialLR
#    gamma: 0.94
#
#policies:
#    - lr_scheduler:
#        instance_name: training_lr
#      starting_epoch: 0
#      ending_epoch: 200
#      frequency: 2
