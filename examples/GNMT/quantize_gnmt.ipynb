{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing Neural Machine Translation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue our quest to quantize every Neural Network!  \n",
    "On this chapter: __Google's Neural Machine Translation model__.  \n",
    "A brief summary - using stacked LSTMs and attention mechanism, this model encodes a sentence into a list of vectors and then decodes it to the other language tokens until an end token is reached.  \n",
    "To read more - refer to <a id=\"ref-1\" href=\"#cite-wu2016google\">Google's paper</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Quantizing Neural Machine Translation Models](#Quantizing-Neural-Machine-Translation-Models)\n",
    "\t* [Getting the resources](#Getting-the-resources)\n",
    "\t* [Loading the model](#Loading-the-model)\n",
    "\t* [Evaulation of the model](#Evaulation-of-the-model)\n",
    "\t* [Quantizing the model](#Quantizing-the-model)\n",
    "\t\t* [Collecting the statistics](#Collecting-the-statistics)\n",
    "\t\t* [Defining the Quantizer](#Defining-the-Quantizer)\n",
    "\t\t* [Quantizing the model](#Quantizing-the-model)\n",
    "\t\t* [Evaluating the quantized model](#Evaluating-the-quantized-model)\n",
    "\t\t* [Finding the right quantization](#Finding-the-right-quantization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we modified the [`mlperf/training/rnn_translator`](https://github.com/mlperf/training/tree/master/rnn_translator) project to enable quantization of the GNMT model.  \n",
    "The instructions to download and setup the required environment for this task are in `README.md` (located in the current directory).  \n",
    "Download the pretrained model using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to download the pretrained model:\n",
    "#! wget https://zenodo.org/record/2581623/files/model_best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have everything ready to start quantizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import distiller\n",
    "from distiller.modules import DistillerLSTM\n",
    "from distiller.quantization import PostTrainLinearQuantizer\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from copy import deepcopy\n",
    "\n",
    "from seq2seq import models\n",
    "from seq2seq.inference.inference import Translator\n",
    "from seq2seq.utils import AverageMeter\n",
    "import subprocess\n",
    "import os\n",
    "import seq2seq.data.config as config\n",
    "from seq2seq.data.dataset import ParallelDataset\n",
    "import logging\n",
    "from seq2seq.utils import AverageMeter\n",
    "# Import utilities from the example:\n",
    "from translate import grouper, write_output, checkpoint_from_distributed, unwrap_distributed\n",
    "from itertools import takewhile\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.disable(logging.INFO)  # Disables mlperf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "batch_first=True\n",
    "batch_size=128\n",
    "beam_size=10\n",
    "cov_penalty_factor=0.1\n",
    "dataset_dir='./data'\n",
    "input='./data/newstest2014.tok.clean.bpe.32000.en'\n",
    "len_norm_const=5.0\n",
    "len_norm_factor=0.6\n",
    "max_seq_len=80\n",
    "model='model_best.pth'\n",
    "output='output_file'\n",
    "print_freq=1\n",
    "reference='./data/newstest2014.de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "checkpoint = torch.load('./model_best.pth', map_location={'cuda:0': 'cpu'})\n",
    "vocab_size = checkpoint['tokenizer'].vocab_size\n",
    "model_config = dict(vocab_size=vocab_size, math=checkpoint['config'].math,\n",
    "                    **literal_eval(checkpoint['config'].model_config))\n",
    "model_config['batch_first'] = batch_first\n",
    "model = models.GNMT(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNMT(\n",
       "  (encoder): ResidualRecurrentEncoder(\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(1024, 1024, batch_first=True)\n",
       "      (3): LSTM(1024, 1024, batch_first=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (embedder): Embedding(32317, 1024, padding_idx=0)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): EltwiseAdd()\n",
       "      (1): EltwiseAdd()\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResidualRecurrentDecoder(\n",
       "    (att_rnn): RecurrentAttention(\n",
       "      (rnn): LSTM(1024, 1024, batch_first=True)\n",
       "      (attn): BahdanauAttention(\n",
       "        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0)\n",
       "        (eltwiseadd_qk): EltwiseAdd()\n",
       "        (eltwiseadd_norm_bias): EltwiseAdd()\n",
       "        (eltwisemul_norm_scaler): EltwiseMult()\n",
       "        (tanh): Tanh()\n",
       "        (matmul_score): Matmul()\n",
       "        (softmax_att): Softmax()\n",
       "        (context_matmul): BatchMatmul()\n",
       "      )\n",
       "      (dropout): Dropout(p=0)\n",
       "    )\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): LSTM(2048, 1024, batch_first=True)\n",
       "      (1): LSTM(2048, 1024, batch_first=True)\n",
       "      (2): LSTM(2048, 1024, batch_first=True)\n",
       "    )\n",
       "    (embedder): Embedding(32317, 1024, padding_idx=0)\n",
       "    (classifier): Classifier(\n",
       "      (classifier): Linear(in_features=1024, out_features=32317, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): EltwiseAdd()\n",
       "      (1): EltwiseAdd()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "if checkpoint_from_distributed(state_dict):\n",
    "    state_dict = unwrap_distributed(state_dict)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "torch.cuda.set_device(0)\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = checkpoint['tokenizer']\n",
    "\n",
    "\n",
    "test_data = ParallelDataset(\n",
    "    src_fname=os.path.join(dataset_dir, config.SRC_TEST_FNAME),\n",
    "    tgt_fname=os.path.join(dataset_dir, config.TGT_TEST_FNAME),\n",
    "    tokenizer=tokenizer,\n",
    "    min_len=0,\n",
    "    max_len=150,\n",
    "    sort=False)\n",
    "\n",
    "def get_loader():\n",
    "    return test_data.get_loader(batch_size=batch_size,\n",
    "                                   batch_first=True,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   drop_last=False,\n",
    "                                   distributed=False)\n",
    "def get_translator(model):\n",
    "    return Translator(model,\n",
    "                       tokenizer,\n",
    "                       beam_size=beam_size,\n",
    "                       max_seq_len=max_seq_len,\n",
    "                       len_norm_factor=len_norm_factor,\n",
    "                       len_norm_const=len_norm_const,\n",
    "                       cov_penalty_factor=cov_penalty_factor,\n",
    "                       cuda=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_path):\n",
    "    test_file = open(test_path, 'w', encoding='UTF-8')\n",
    "    model.eval()\n",
    "    translator = get_translator(model)\n",
    "    stats = {}\n",
    "    iterations = enumerate(tqdm(get_loader()))\n",
    "    for i, (src, tgt, indices) in iterations:\n",
    "        src, src_length = src\n",
    "        if translator.batch_first:\n",
    "            batch_size = src.size(0)\n",
    "        else:\n",
    "            batch_size = src.size(1)\n",
    "        bos = [translator.insert_target_start] * (batch_size * beam_size)\n",
    "        bos = torch.LongTensor(bos)\n",
    "        if translator.batch_first:\n",
    "            bos = bos.view(-1, 1)\n",
    "        else:\n",
    "            bos = bos.view(1, -1)\n",
    "        src_length = torch.LongTensor(src_length)\n",
    "        stats['total_enc_len'] = int(src_length.sum())\n",
    "        src = src.cuda()\n",
    "        src_length = src_length.cuda()\n",
    "        bos = bos.cuda()\n",
    "        with torch.no_grad():\n",
    "            context = translator.model.encode(src, src_length)\n",
    "            context = [context, src_length, None]\n",
    "            if beam_size == 1:\n",
    "                generator = translator.generator.greedy_search\n",
    "            else:\n",
    "                generator = translator.generator.beam_search\n",
    "            preds, lengths, counter = generator(batch_size, bos, context)\n",
    "        stats['total_dec_len'] = lengths.sum().item()\n",
    "        stats['iters'] = counter\n",
    "        preds = preds.cpu()\n",
    "        lengths = lengths.cpu()\n",
    "        output = []\n",
    "        for idx, pred in enumerate(preds):\n",
    "            end = lengths[idx] - 1\n",
    "            pred = pred[1: end]\n",
    "            pred = pred.tolist()\n",
    "            out = translator.tok.detokenize(pred)\n",
    "            output.append(out)\n",
    "        output = [output[indices.index(i)] for i in range(len(output))]\n",
    "        for line in output:\n",
    "            test_file.write(line)\n",
    "            test_file.write('\\n')\n",
    "        total_tokens = stats['total_dec_len'] + stats['total_enc_len']\n",
    "    test_file.close()\n",
    "    # run moses detokenizer\n",
    "    detok_path = os.path.join(dataset_dir, config.DETOKENIZER)\n",
    "    detok_test_path = test_path + '.detok'\n",
    "\n",
    "    with open(detok_test_path, 'w') as detok_test_file, \\\n",
    "            open(test_path, 'r') as test_file:\n",
    "        subprocess.run(['perl', detok_path], stdin=test_file,\n",
    "                       stdout=detok_test_file, stderr=subprocess.DEVNULL)\n",
    "    # run sacrebleu\n",
    "    reference_path = os.path.join(dataset_dir,\n",
    "                                  config.TGT_TEST_TARGET_FNAME)\n",
    "    sacrebleu = subprocess.run(['sacrebleu --input {} {} --score-only -lc --tokenize intl'.\n",
    "                                format(detok_test_path, reference_path)],\n",
    "                               stdout=subprocess.PIPE, shell=True)\n",
    "    bleu = float(sacrebleu.stdout.strip())\n",
    "    print('BLEU on test dataset: {}'.format(bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:54<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 22.16\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already noted, we modified the model from `mlperf` to a modular implementation so we can quantize each and every operation in the graph.  \n",
    "However, the default `nn.LSTM` was implemented in C++/CUDA, and we don't have usual access to it's operations hence we can't quantize it properly. This is why we'll convert the `nn.LSTM` to a `DistillerLSTM`, which is an entirely modular implementation of the LSTM - identical in functionality to the original `nn.LSTM`.  \n",
    "This is done by simply calling `DistillerLSTM.from_pytorch_impl` for a single `nn.LSTM` and  \n",
    "`convert_model_to_distiller_lstm` for an entire model containing multiple different LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [02:25<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 22.16\n"
     ]
    }
   ],
   "source": [
    "from distiller.modules import convert_model_to_distiller_lstm\n",
    "model = convert_model_to_distiller_lstm(model)\n",
    "evaluate(model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantizer uses statistics to define the range of the quantization. We collect these statistics using a `QuantCalibrationStatsCollector` instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "stats_file = './model_stats.yaml'\n",
    "\n",
    "if not os.path.isfile(stats_file): # Collect stats.\n",
    "    model_copy = deepcopy(model)\n",
    "    distiller.utils.assign_layer_fq_names(model_copy)\n",
    "    collector = QuantCalibrationStatsCollector(model_copy)\n",
    "    with collector_context(collector):\n",
    "        val_loss = evaluate(model_copy, output + '.temp')\n",
    "    collector.save(stats_file)\n",
    "    del model_copy\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distiller `Quantizer` object replaces each submodule in a model with its quantized counterpart, using a \n",
    "`replacement_factory`.  \n",
    "`Quantizer.replacement_factory` is a dictionary which maps from a module type (e.g. `nn.Linear` and `nn.Conv`) to a function. This function takes a module and quantization configuration, and returns a quantized version of the same module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 'Conv2d' modules using 'replace_param_layer' function\n",
      "Replacing 'Linear' modules using 'replace_param_layer' function\n",
      "Replacing 'BatchMatmul' modules using 'replace_non_param_layer' function\n",
      "Replacing 'EltwiseAdd' modules using 'replace_non_param_layer' function\n",
      "Replacing 'Concat' modules using 'replace_non_param_layer' function\n",
      "Replacing 'Embedding' modules using 'replace_embedding' function\n",
      "Replacing 'Matmul' modules using 'replace_non_param_layer' function\n",
      "Replacing 'EltwiseMult' modules using 'replace_non_param_layer' function\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"SYMMETRIC\",  # As was suggested in GNMT's paper\n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides)\n",
    "# We take a look at the replacement factory:\n",
    "for t, rf in quantizer.replacement_factory.items():\n",
    "    print(\"Replacing '{}' modules using '{}' function\".format(t.__name__, rf.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done by simply calling `quantizer.prepare_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0731 14:38:28.720480 140149623748352 range_linear.py:1063] /home/cvds_lab/lev_z/distiller/distiller/quantization/range_linear.py:1063: UserWarning: Model contains a bidirectional DistillerLSTM module. Automatic BN folding and statistics optimization based on tracing is not yet supported for models containing such modules.\n",
      "Will perform specific optimization for the DistillerLSTM modules, but any other potential opportunities for optimization in the model will be ignored.\n",
      "  'opportunities for optimization in the model will be ignored.', UserWarning)\n",
      "\n",
      "W0731 14:38:28.846097 140149623748352 quantizer.py:270] /home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:270: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNMT(\n",
       "  (encoder): ResidualRecurrentEncoder(\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=True)\n",
       "      (1): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (2): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (3): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (embedder): RangeLinearEmbeddingWrapper(\n",
       "      (wrapped_module): Embedding(32317, 1024, padding_idx=0)\n",
       "    )\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00788879394531, in_0_zero_point=0.0\n",
       "        in_1_scale=127.00006103515625, in_1_zero_point=0.0\n",
       "        out_scale=63.530452728271484, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "      (1): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00004577636719, in_0_zero_point=0.0\n",
       "        in_1_scale=63.530452728271484, in_1_zero_point=0.0\n",
       "        out_scale=42.43059158325195, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResidualRecurrentDecoder(\n",
       "    (att_rnn): RecurrentAttention(\n",
       "      (rnn): DistillerLSTM(1024, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (attn): BahdanauAttention(\n",
       "        (linear_q): RangeLinearQuantParamLayerWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          w_scale=53.0649, w_zero_point=0.0000\n",
       "          in_scale=127.0000, in_zero_point=0.0000\n",
       "          out_scale=4.2200, out_zero_point=0.0000\n",
       "          (wrapped_module): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (linear_k): RangeLinearQuantParamLayerWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          w_scale=40.9795, w_zero_point=0.0000\n",
       "          in_scale=42.4306, in_zero_point=0.0000\n",
       "          out_scale=4.5466, out_zero_point=0.0000\n",
       "          (wrapped_module): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0)\n",
       "        (eltwiseadd_qk): RangeLinearQuantEltwiseAddWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=4.219996452331543, in_0_zero_point=0.0\n",
       "          in_1_scale=4.546571731567383, in_1_zero_point=0.0\n",
       "          out_scale=2.974982261657715, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseAdd()\n",
       "        )\n",
       "        (eltwiseadd_norm_bias): RangeLinearQuantEltwiseAddWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=2.974982261657715, in_0_zero_point=0.0\n",
       "          in_1_scale=109.55178833007812, in_1_zero_point=0.0\n",
       "          out_scale=2.961179256439209, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseAdd()\n",
       "        )\n",
       "        (eltwisemul_norm_scaler): RangeLinearQuantEltwiseMultWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=771.8616333007812, in_0_zero_point=0.0\n",
       "          in_1_scale=100.56507873535156, in_1_zero_point=0.0\n",
       "          out_scale=611.1993408203125, out_zero_point=0.0\n",
       "          (wrapped_module): EltwiseMult()\n",
       "        )\n",
       "        (tanh): Tanh()\n",
       "        (matmul_score): RangeLinearQuantMatmulWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8,  num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=127.0000, in_0_zero_point=0.0000\n",
       "          in_1_scale=611.1993, in_1_zero_point=0.0000\n",
       "          out_scale=6.3274, out_zero_point=0.0000\n",
       "          (wrapped_module): Matmul()\n",
       "        )\n",
       "        (softmax_att): Softmax()\n",
       "        (context_matmul): RangeLinearQuantMatmulWrapper(\n",
       "          mode=SYMMETRIC, num_bits_acts=8,  num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "          preset_activation_stats=True\n",
       "          in_0_scale=128.1420, in_0_zero_point=0.0000\n",
       "          in_1_scale=42.4306, in_1_zero_point=0.0000\n",
       "          out_scale=46.2056, out_zero_point=0.0000\n",
       "          (wrapped_module): BatchMatmul()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0)\n",
       "    )\n",
       "    (rnn_layers): ModuleList(\n",
       "      (0): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (1): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "      (2): DistillerLSTM(2048, 1024, num_layers=1, dropout=0.00, bidirectional=False)\n",
       "    )\n",
       "    (embedder): RangeLinearEmbeddingWrapper(\n",
       "      (wrapped_module): Embedding(32317, 1024, padding_idx=0)\n",
       "    )\n",
       "    (classifier): Classifier(\n",
       "      (classifier): RangeLinearQuantParamLayerWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=NONE, per_channel_wts=False, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        w_scale=40.8311, w_zero_point=0.0000\n",
       "        in_scale=42.8144, in_zero_point=0.0000\n",
       "        out_scale=6.3242, out_zero_point=0.0000\n",
       "        (wrapped_module): Linear(in_features=1024, out_features=32317, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2)\n",
       "    (eltwiseadd_residuals): ModuleList(\n",
       "      (0): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.01639556884766, in_0_zero_point=0.0\n",
       "        in_1_scale=127.00018310546875, in_1_zero_point=0.0\n",
       "        out_scale=63.68953323364258, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "      (1): RangeLinearQuantEltwiseAddWrapper(\n",
       "        mode=SYMMETRIC, num_bits_acts=8, num_bits_accum=32, clip_acts=NONE, scale_approx_mult_bits=None\n",
       "        preset_activation_stats=True\n",
       "        in_0_scale=127.00001525878906, in_0_zero_point=0.0\n",
       "        in_1_scale=63.68953323364258, in_1_zero_point=0.0\n",
       "        out_scale=42.81440353393555, out_zero_point=0.0\n",
       "        (wrapped_module): EltwiseAdd()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = (torch.ones(1, 2).to(dtype=torch.long),\n",
    "               torch.ones(1).to(dtype=torch.long),\n",
    "               torch.ones(1, 2).to(dtype=torch.long))\n",
    "quantizer.prepare_model(dummy_input)\n",
    "quantizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to know how these functions replace the modules - I recommend reading the source code for them in  \n",
    "`{DISTILLER_ROOT}/distiller/quantization/range_linear.py:PostTrainLinearQuantizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [13:27<00:00, 28.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 18.09\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, we quantized our model entirely and it lost some accuracy, so we want to apply more strategies to quantize better.  \n",
    "Symmetric quantization means our range is the biggest we can hold our activations in:\n",
    "$$\n",
    "    M = \\max \\{ |\\text{acts}|\\},\\, \\text{range}_{symmetric} = [-M, M]\n",
    "$$\n",
    "This way we waste resolution. However, if we use assymetric quantization - we may get better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 15:03:20.999896 140149623748352 range_linear.py:1063] /home/cvds_lab/lev_z/distiller/distiller/quantization/range_linear.py:1063: UserWarning: Model contains a bidirectional DistillerLSTM module. Automatic BN folding and statistics optimization based on tracing is not yet supported for models containing such modules.\n",
      "Will perform specific optimization for the DistillerLSTM modules, but any other potential opportunities for optimization in the model will be ignored.\n",
      "  'opportunities for optimization in the model will be ignored.', UserWarning)\n",
      "\n",
      "W0731 15:03:21.123406 140149623748352 quantizer.py:270] /home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:270: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n",
      "\n",
      "100%|██████████| 24/24 [15:04<00:00, 33.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 18.51\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"ASYMMETRIC_SIGNED\",  \n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides)\n",
    "quantizer.prepare_model()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here - we quantized asymmetrically, meaning our range still holds all the activations, but it's smaller than in the symmetrical case.  \n",
    "The formula is:\n",
    "$$\n",
    "    \\text{range}_{asymmetric} = \\left[\\min\\{ \\text{acts}\\}, \\max \\{ \\text{acts}\\}\\right] \n",
    "    \\subset \\text{range}_{symmetric}\n",
    "$$\n",
    "And we indeed got a slightly better result.  \n",
    "However - some part of the activations during the evaluations are outliers, meaning they are way outside the range of most of their buddies. We're going to intercept this in two ways -\n",
    "1. Quantize each channel separately, that way we achieve more accuracy. We'll add the argument `per_channel_wts=True`.\n",
    "2. Limit the quantization range to a smaller one, thus clamping these outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try using the same technique as in `quantize_lstm.ipynb` - clipping the activations according to the average range recorded for them:  \n",
    "\n",
    "\n",
    "$$\n",
    "    m = \\underset{b\\in\\text{batches}}{\\text{avg}}\\left\\{\\min_{b}\\{\\text{acts}\\}\\right\\},\\,\n",
    "    M = \\underset{b\\in\\text{batches}}{\\text{avg}}\\left\\{\\max_{b}\\{\\text{acts}\\}\\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    \\text{range}_{clipped} = [m,M] \\subset \\text{range}_{asymmetric} \\subset \\text{range}_{symmetric}\n",
    "$$\n",
    "\n",
    "This is done by specifying `clip_acts=\"AVG\"` in the quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 15:18:32.016489 140149623748352 range_linear.py:1063] /home/cvds_lab/lev_z/distiller/distiller/quantization/range_linear.py:1063: UserWarning: Model contains a bidirectional DistillerLSTM module. Automatic BN folding and statistics optimization based on tracing is not yet supported for models containing such modules.\n",
      "Will perform specific optimization for the DistillerLSTM modules, but any other potential opportunities for optimization in the model will be ignored.\n",
      "  'opportunities for optimization in the model will be ignored.', UserWarning)\n",
      "\n",
      "W0731 15:18:33.228340 140149623748352 quantizer.py:270] /home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:270: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n",
      "\n",
      "100%|██████████| 24/24 [17:03<00:00, 35.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 1.28\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"ASYMMETRIC_SIGNED\",  \n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides,\n",
    "                                    per_channel_wts=True,\n",
    "                                    clip_acts=\"AVG\")\n",
    "quantizer.prepare_model()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! This is bad... turns out that by clamping the outliers we actually \"removed\" useful features from important layers like the attention layer. In the attention layer we have a softmax which relies on high values to pass a correct score of importance of features. Let's try clipping all the other values, except in the attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 15:35:44.383689 140149623748352 range_linear.py:1063] /home/cvds_lab/lev_z/distiller/distiller/quantization/range_linear.py:1063: UserWarning: Model contains a bidirectional DistillerLSTM module. Automatic BN folding and statistics optimization based on tracing is not yet supported for models containing such modules.\n",
      "Will perform specific optimization for the DistillerLSTM modules, but any other potential opportunities for optimization in the model will be ignored.\n",
      "  'opportunities for optimization in the model will be ignored.', UserWarning)\n",
      "\n",
      "W0731 15:35:45.713025 140149623748352 quantizer.py:270] /home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:270: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n",
      "\n",
      "100%|██████████| 24/24 [15:28<00:00, 33.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 16.87\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    ".*att_rnn.attn.*:\n",
    "    clip_acts: NONE # Quantize without clipping\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"ASYMMETRIC_SIGNED\",  \n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides,\n",
    "                                    per_channel_wts=True,\n",
    "                                    clip_acts=\"AVG\")\n",
    "quantizer.prepare_model()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is somewhat \"restored\", by still we would like to get a score as close to the original model as possible. How about leaving the `classifier` asymmetric, without clipping it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0731 15:51:22.080519 140149623748352 range_linear.py:1063] /home/cvds_lab/lev_z/distiller/distiller/quantization/range_linear.py:1063: UserWarning: Model contains a bidirectional DistillerLSTM module. Automatic BN folding and statistics optimization based on tracing is not yet supported for models containing such modules.\n",
      "Will perform specific optimization for the DistillerLSTM modules, but any other potential opportunities for optimization in the model will be ignored.\n",
      "  'opportunities for optimization in the model will be ignored.', UserWarning)\n",
      "\n",
      "W0731 15:51:23.896918 140149623748352 quantizer.py:270] /home/cvds_lab/lev_z/distiller/distiller/quantization/quantizer.py:270: UserWarning: Module 'decoder.embedder' references to same module as 'encoder.embedder'. Replacing with reference the same wrapper.\n",
      "  UserWarning)\n",
      "\n",
      "100%|██████████| 24/24 [14:08<00:00, 28.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU on test dataset: 21.53\n"
     ]
    }
   ],
   "source": [
    "# We quantize everything except softmax\n",
    "overrides_yaml = \"\"\"\n",
    ".*softmax.*: \n",
    "    bits_weights: null\n",
    "    bits_activations: null\n",
    ".*att_rnn.attn.*:\n",
    "    clip_acts: NONE # Quantize without clipping\n",
    "decoder.classifier.classifier:\n",
    "    clip_acts: NONE # Quantize without clipping\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "# Basic quantizer defintion\n",
    "quantizer = PostTrainLinearQuantizer(deepcopy(model), \n",
    "                                    mode=\"ASYMMETRIC_SIGNED\",  \n",
    "                                    model_activation_stats=stats_file,\n",
    "                                    overrides=overrides,\n",
    "                                    per_channel_wts=True,\n",
    "                                    clip_acts=\"AVG\")\n",
    "quantizer.prepare_model()\n",
    "evaluate(quantizer.model, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some good results! So now we know better which layers are sensitive to clipping and which are complimented by it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id=\"cite-wu2016google\"/><sup><a href=#ref-1>[^]</a></sup>Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others. 2016. _Google's neural machine translation system: Bridging the gap between human and machine translation_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@article{wu2016google,\n",
    "  title={Google's neural machine translation system: Bridging the gap between human and machine translation},\n",
    "  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},\n",
    "  journal={arXiv preprint arXiv:1609.08144},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
