{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing RNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show how to quantize recurrent models. <br />\n",
    "Using a pretrained model `model.RNNModel`, we convert the built-in pytorch implementation of LSTM to our own, modular, implementation. <br />\n",
    "The reason we do that is because the inner operations in the pytorch implementation are not accessible to us, but we still want to quantize these operations. <br />\n",
    "Afterwards we can try different techniques to quantize the whole model.  \n",
    "\n",
    "_NOTE_: We use `tqdm` to plot progress bars, since it's not in `requirements.txt` you should install it using \n",
    "`pip install tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WordLangModel, RNNModel\n",
    "from data import Corpus\n",
    "import torch\n",
    "from torch import nn\n",
    "import distiller\n",
    "from distiller.modules import LSTM\n",
    "from tqdm import tqdm # for pretty progress bar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('./data/wikitext-2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "device = 'cuda:0'\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and converting to our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.65)\n",
       "  (encoder): Embedding(33278, 1500)\n",
       "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)\n",
       "  (decoder): Linear(in_features=1500, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = torch.load('./checkpoint.pth.tar.best')\n",
    "rnn_model = rnn_model.to(device)\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the pytorch LSTM implementation to our own, by calling `LSTM.from_pytorch_impl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordLangModel(\n",
       "  (encoder): Embedding(33278, 1500)\n",
       "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
       "  (decoder): Linear(in_features=1500, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_model(pytorch_model_: RNNModel):\n",
    "    nlayers, ninp, nhid, ntoken, tie_weights = \\\n",
    "        pytorch_model_.nlayers, \\\n",
    "        pytorch_model_.ninp, \\\n",
    "        pytorch_model_.nhid, \\\n",
    "        pytorch_model_.ntoken, \\\n",
    "        pytorch_model_.tie_weights\n",
    "\n",
    "    model = WordLangModel(nlayers=nlayers, ninp=ninp, nhid=nhid, ntoken=ntoken, tie_weights=tie_weights).to(device)\n",
    "    model.eval()\n",
    "    model.encoder.weight = nn.Parameter(pytorch_model_.encoder.weight.clone().detach())\n",
    "    model.decoder.weight = nn.Parameter(pytorch_model_.decoder.weight.clone().detach())\n",
    "    model.decoder.bias = nn.Parameter(pytorch_model_.decoder.bias.clone().detach())\n",
    "    model.rnn = LSTM.from_pytorch_impl(pytorch_model_.rnn)\n",
    "\n",
    "    return model\n",
    "\n",
    "man_model = manual_model(rnn_model)\n",
    "torch.save(man_model, 'manual.checkpoint.pth.tar')\n",
    "man_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(sequence_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "hidden = rnn_model.init_hidden(eval_batch_size)\n",
    "data, targets = get_batch(test_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the convertion has succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error in y: 0.000011\n"
     ]
    }
   ],
   "source": [
    "rnn_model.eval()\n",
    "man_model.eval()\n",
    "y_t, h_t = rnn_model(data, hidden)\n",
    "y_p, h_p = man_model(data, hidden)\n",
    "\n",
    "print(\"Max error in y: %f\" % (y_t-y_p).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "\n",
    "def evaluate(model, data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        # The line below was fixed as per: https://github.com/pytorch/examples/issues/214\n",
    "        for i in tqdm(range(0, data_source.size(0), sequence_len)):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activation statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses activation statistics to determine how big the quants are, using the formula:  \n",
    "\n",
    "$$\n",
    "    q = \\frac{\\max\\limits_{x\\in X}\\{ \\sigma(x) \\} - \\min\\limits_{x\\in X}\\{ \\sigma(x) \\}}{2^{bits}-1}\n",
    "$$\n",
    "\n",
    "\n",
    "The class `QuantCalibrationStatsCollector` collects the statistics for defining the range $r = max - min$.  \n",
    "\n",
    "Each forward pass, the collector records the values of inputs and outputs, for each layer:\n",
    "- absolute over all batches min, max (stored in `min`, `max`)\n",
    "- average over batches, per batch min, max (stored in `avg_min`, `avg_max`)\n",
    "- mean\n",
    "- std\n",
    "- shape of output tensor  \n",
    "\n",
    "All these values can be used to define the range of quantization, e.g. we can use the absolute `min`, `max` to define the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from distiller.data_loggers import QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "distiller.utils.assign_layer_fq_names(man_model)\n",
    "collector = QuantCalibrationStatsCollector(man_model)\n",
    "\n",
    "if not os.path.isfile('manual_lstm_pretrained_stats.yaml'):\n",
    "    with collector_context(collector) as collector:\n",
    "        val_loss = evaluate(man_model, val_data)\n",
    "        collector.save('manual_lstm_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Model:\n",
    "  \n",
    "We quantize the model after the training has completed.  \n",
    "Here we check the baseline model perplexity, to have an idea how good the quantization is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [00:23<00:00, 26.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:    4.46\t|\t ppl:   86.78\n"
     ]
    }
   ],
   "source": [
    "from distiller.quantization import PostTrainLinearQuantizer, LinearQuantMode\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load and evaluate the baseline model.\n",
    "man_model = torch.load('./manual.checkpoint.pth.tar')\n",
    "val_loss = evaluate(man_model, val_data)\n",
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))\n",
    "\n",
    "# Define the quantizer\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [00:19<00:00, 31.81it/s]\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(rnn_model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:    4.46\t|\t ppl:   86.78\n"
     ]
    }
   ],
   "source": [
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do our magic - __Quantizing the model__.  \n",
    "The quantizer replaces the layers in out model with their quantized versions.  \n",
    "We can see that our model has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantizer magic:\n",
    "quantizer.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordLangModel(\n",
       "  (encoder): RangeLinearEmbeddingWrapper(\n",
       "    (wrapped_module): Embedding(33278, 1500)\n",
       "  )\n",
       "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
       "  (decoder): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=SYMMETRIC, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=False, per_channel_wts=False\n",
       "    preset_activation_stats=True\n",
       "    w_scale=126.2964, w_zero_point=0.0000\n",
       "    in_scale=127.0004, in_zero_point=0.0000\n",
       "    out_scale=3.6561, out_zero_point=0.0000\n",
       "    (wrapped_module): Linear(in_features=1500, out_features=33278, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [02:03<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(quantizer.model.to(device), val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:    4.64\t|\t ppl:  103.26\n"
     ]
    }
   ],
   "source": [
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the perplexity has increased much - meaning our quantization has damaged the accuracy of our model.  \n",
    "Let's try quantizing each channel separately, and making the range of the quantization asymmetric.  \n",
    "Also - we replaced the `min`, `max` boundaries manually in the file.  \n",
    "The idea is - the quantizer takes the absolute `min`, `max` boundaries by default, and in the original file many of the activations had a very large range that makes our quants very big - while we want to minimize their size since each quant corresponds to a roundoff error.  \n",
    "The activations in every LSTM are either `sigmoid` or `tanh`, and since these are bounded respectively by\n",
    "$[0,1]$, $[-1,1]$ and they saturate very quickly - we can clip the inputs to be between in the range of $[-6,6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordLangModel(\n",
       "  (encoder): RangeLinearEmbeddingWrapper(\n",
       "    (wrapped_module): Embedding(33278, 1500)\n",
       "  )\n",
       "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
       "  (decoder): RangeLinearQuantParamLayerWrapper(\n",
       "    mode=ASYMMETRIC_SIGNED, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=False, per_channel_wts=True\n",
       "    preset_activation_stats=True\n",
       "    w_scale=PerCh, w_zero_point=PerCh\n",
       "    in_scale=127.5069, in_zero_point=1.0000\n",
       "    out_scale=5.0241, out_zero_point=48.0000\n",
       "    (wrapped_module): Linear(in_features=1500, out_features=33278, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    per_channel_wts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "quantizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [02:14<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:    4.61\t|\t ppl:  100.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.2f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tiny bit better, but still no good. Let us try the half precision version of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [00:28<00:00, 21.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 4.463242\t|\t ppl:   86.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = deepcopy(man_model).half()\n",
    "val_loss = evaluate(model_fp16, val_data)\n",
    "print('val_loss: %8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is very close to our original model! That means that the roundoff when quantizing lineary is what hurts our accuracy. Let's try then quantizing everything except elemtentwise operations, as stated in \n",
    "[`Effective Quantization Methods for Recurrent Neural Networks`](https://arxiv.org/abs/1611.10176) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/622 [00:00<01:19,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLangModel(\n",
      "  (encoder): FP16Wrapper(\n",
      "    (wrapped_module): Embedding(33278, 1500)\n",
      "  )\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
      "  (decoder): FP16Wrapper(\n",
      "    (wrapped_module): Linear(in_features=1500, out_features=33278, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [01:20<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:4.463708\t|\t ppl:   86.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "overrides_yaml = \"\"\"\n",
    ".*eltwise.*:\n",
    "    fp16: true\n",
    "encoder:\n",
    "    fp16: true\n",
    "decoder:\n",
    "    fp16: true\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    overrides=overrides,\n",
    "    per_channel_wts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "print(quantizer.model)\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is still holding up very well, even though we quantized the inner linear layers!  \n",
    "Now, lets try to choose different boundaries for `min`, `max` -  \n",
    "Instead of using absolute ones, we take the average of all batches (`avg_min`, `avg_max`), which is an indication of where usually most of the boundaries lie. This is done by specifying the `clip_acts` flag in the quantizer ctor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/622 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLangModel(\n",
      "  (encoder): FP16Wrapper(\n",
      "    (wrapped_module): Embedding(33278, 1500)\n",
      "  )\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
      "  (decoder): FP16Wrapper(\n",
      "    (wrapped_module): Linear(in_features=1500, out_features=33278, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [02:09<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:4.487813\t|\t ppl:   88.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "overrides_yaml = \"\"\"\n",
    "encoder:\n",
    "    fp16: true\n",
    "decoder:\n",
    "    fp16: true\n",
    "\"\"\"\n",
    "overrides = distiller.utils.yaml_ordered_load(overrides_yaml)\n",
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    overrides=overrides,\n",
    "    per_channel_wts=True,\n",
    "    clip_acts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "print(quantizer.model)\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Even though we quantized all of the layers except the embedding and the decoder - we got almost no accuracy penalty. Lets try quantizing them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/622 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLangModel(\n",
      "  (encoder): RangeLinearEmbeddingWrapper(\n",
      "    (wrapped_module): Embedding(33278, 1500)\n",
      "  )\n",
      "  (rnn): LSTM(1500, 1500, num_layers=2, dropout=    0.65)\n",
      "  (decoder): RangeLinearQuantParamLayerWrapper(\n",
      "    mode=ASYMMETRIC_SIGNED, num_bits_acts=8, num_bits_params=8, num_bits_accum=32, clip_acts=True, per_channel_wts=True\n",
      "    preset_activation_stats=True\n",
      "    w_scale=PerCh, w_zero_point=PerCh\n",
      "    in_scale=129.4670, in_zero_point=1.0000\n",
      "    out_scale=9.9393, out_zero_point=56.0000\n",
      "    (wrapped_module): Linear(in_features=1500, out_features=33278, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 622/622 [02:12<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss:4.487492\t|\t ppl:   88.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quantizer = PostTrainLinearQuantizer(\n",
    "    deepcopy(man_model),\n",
    "    model_activation_stats='./manual_lstm_pretrained_stats_new.yaml',\n",
    "    mode=LinearQuantMode.ASYMMETRIC_SIGNED,\n",
    "    per_channel_wts=True,\n",
    "    clip_acts=True\n",
    ")\n",
    "quantizer.prepare_model()\n",
    "print(quantizer.model)\n",
    "val_loss = evaluate(quantizer.model.to(device), val_data)\n",
    "print('val_loss:%8.6f\\t|\\t ppl:%8.2f' % (val_loss, np.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that sometimes quantizing with the right boundaries gives better results than actually using floating point operations (even though they are half precision). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Choosing the right boundaries for quantization  was crucial for achieving almost no degradation in accrucay of LSTM.  \n",
    "  \n",
    "Here we showed how to use the distiller quantization API to quantize an RNN model, by converting the pytorch implementation into a modular one and then quantizing each layer separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
