quantizers:
  linear_quantizer:
    class: NCFQuantAwareTrainQuantizer
    bits_activations: 8
    bits_weights: 8
    bits_bias: 32
    mode: 'SYMMETRIC'  # Can try "SYMMETRIC" as well
    ema_decay: 0.999   # Decay value for exponential moving average tracking of activation ranges
    per_channel_wts: True
    overrides:
      # We want to quantize the last FC layer prior to the sigmoid. So - We set up the quantizer to add fake-quantization
      # layers after FC layers. But - here we override so that this doesn't actually happen in any of the early FC layers
      mlp\.*:
        bits_activations: null
        bits_weights: 8
        bits_bias: 32
      final_concat:
        bits_activations: null
        bits_weights: null
        bits_bias: null

policies:
    - quantizer:
        instance_name: linear_quantizer
      # For now putting a large range here, which should cover both training from scratch or resuming from some
      # pre-trained checkpoint at some unknown epoch
      starting_epoch: 0
      ending_epoch: 300
      frequency: 1