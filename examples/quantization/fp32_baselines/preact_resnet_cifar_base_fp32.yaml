# Scheduler for training baseline FP32 models of pre-activation ResNet on CIFAR-10
# Applicable to ResNet 20 / 32 / 44 / 56 / 110
# 
# Command line for training (running from the compress_classifier.py directory):
# python compress_classifier.py -a preact_resnet20_cifar --lr 0.1 -p 50 -b 128 <path_to_cifar10_dataset> -j 1 --epochs 200 --compress=../quantization/fp32_baselines/preact_resnet_cifar_base_fp32.yaml --wd=0.0002 --vs=0 --gpus 0
#
# Notes:
#  * In '-a preact_resnet20_cifar', replace '20' with the required depth
#  * '--wd=0.0002': Weight decay of 0.0002 is used
#  * '--vs=0': We train on the entire training dataset, and validate using the test set
#
# Knowledge Distillation:
# -----------------------
# To train these models with knowledge distillation, add the following arguments to the command line:
# --kd-teacher preact_resnet44_cifar --kd-resume <path_to_teacher_model_checkpoint> --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3
#
# Notes:
#  * Replace 'preact_resnet44_cifar' with the required teacher model
#  * Use the command line at the top to train teacher models, which can be used as checkpoints passed to '--kd-resume'
#    (the shell script we point to below can be used to train baseline teacher models)
#  * More details on knowledge distillation at: 
#    https://intellabs.github.io/distiller/schedule.html#knowledge-distillation
#
# See some experimental results below after the YAML schedule

lr_schedulers:
  training_lr:
    class: MultiStepMultiGammaLR
    milestones: [80, 120, 160]
    gammas: [0.1, 0.1, 0.2]

policies:
    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1

# The results listed here are based on 4 runs in each configuration. All results are Top-1.
# 
# Notes:
#  * In this example we're testing three distillation temperatures: 1.0, 2.0, 5.0
#    This is controlled by the '--kd-temp' command line argument.
#  * We give a weight of 0.7 to the distillation loss (that is - the loss of the student predictions vs.
#    the teacher's soft targets) and 0.3 weight to the "standard" studen vs. labels loss.
#    This is done by passing the command line arguments: '--kd-dw 0.7 --kd-sw 0.3'
#  * We don't change any of the other training hyper-parameters
# 
# The shell script used to generate these results is provided at:
# <distiller_root>/examples/quantization/preact_resnet_cifar_quant_distill_tests.sh
#
# (additional results and some limited analysis is included in the preact_resnet_cifar_dorefa.yaml file,
#  located in the same directory as this file)
#
# +-------+-----------------------+-------------------------+
# |       | Distillation Settings |      Results: FP32      |
# +-------+---------+-------------+-------+-------+---------+
# | Depth | Teacher | Temperature | Best  | Worst | Average |
# +-------+---------+-------------+-------+-------+---------+
# | 20    | None    | N/A         | 92.21 | 91.96 | 92.13   |
# |       +---------+-------------+-------+-------+---------+
# |       | 32      | 1           | 92.58 | 92.18 | 92.375  |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 92.92 | 92.61 | 92.775  |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.2  | 93.11 | 93.1725 |
# |       +---------+-------------+-------+-------+---------+
# |       | 44      | 1           | 92.77 | 92.23 | 92.4625 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 93.02 | 92.74 | 92.8625 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.3  | 93.11 | 93.21   |
# |       +---------+-------------+-------+-------+---------+
# |       | 56      | 1           | 92.58 | 92    | 92.2875 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 92.93 | 92.51 | 92.73   |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.11 | 92.82 | 92.96   |
# |       +---------+-------------+-------+-------+---------+
# |       | 110     | 1           | 92.46 | 92.16 | 92.25   |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 92.9  | 92.54 | 92.745  |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.33 | 93    | 93.1475 |
# +-------+---------+-------------+-------+-------+---------+
# | 32    | None    | N/A         | 93.19 | 92.72 | 92.9975 |
# |       +---------+-------------+-------+-------+---------+
# |       | 44      | 1           | 93.43 | 93.22 | 93.3475 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 94.01 | 93.37 | 93.605  |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.9  | 93.42 | 93.68   |
# |       +---------+-------------+-------+-------+---------+
# |       | 56      | 1           | 93.34 | 92.93 | 93.16   |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 93.99 | 93.29 | 93.5925 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.9  | 93.62 | 93.78   |
# |       +---------+-------------+-------+-------+---------+
# |       | 110     | 1           | 93.43 | 93.18 | 93.335  |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 93.89 | 93.5  | 93.6675 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 93.89 | 93.63 | 93.795  |
# +-------+---------+-------------+-------+-------+---------+
# | 44    | None    | N/A         | 93.81 | 93.23 | 93.58   |
# |       +---------+-------------+-------+-------+---------+
# |       | 56      | 1           | 93.97 | 93.57 | 93.7775 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 94.11 | 93.89 | 94.01   |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 94.28 | 93.97 | 94.1225 |
# |       +---------+-------------+-------+-------+---------+
# |       | 110     | 1           | 93.6  | 93.48 | 93.5375 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 94.36 | 93.9  | 94.1075 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 94.39 | 94.04 | 94.225  |
# +-------+---------+-------------+-------+-------+---------+
# | 56    | None    | N/A         | 93.99 | 93.7  | 93.88   |
# |       +---------+-------------+-------+-------+---------+
# |       | 110     | 1           | 94.33 | 93.85 | 94.0675 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 2           | 94.33 | 94.1  | 94.2425 |
# |       |         +-------------+-------+-------+---------+
# |       |         | 5           | 94.41 | 94.18 | 94.3125 |
# +-------+---------+-------------+-------+-------+---------+
# | 110   | None    | N/A         | 94.69 | 94.33 | 94.475  |
# +-------+---------+-------------+-------+-------+---------+
