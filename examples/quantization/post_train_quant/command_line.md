# Post-Training Quantization Command Line Examples

Following are a few examples of invoking post-training quantization on ResNet-50, using Distiller's image classification sample.  

## Available Arguments

| Long Form                | Short     | Description                                                                           | Default |
|--------------------------|-----------|---------------------------------------------------------------------------------------|---------|
| `--quantize-eval`        | `--qe`    | Apply linear quantization to model before evaluation                                  | Off     |
| `--qe-mode`              | `--qem`   | Linear quantization mode. Choices: "sym", "asym_u", "asym_s"                          | "sym"   |
| `--qe-bits-acts`         | `--qeba`  | # of bits for quantization of activations                                             | 8       |
| `--qe-bits-wts`          | `--qebw`  | # of bits for quantization of weights                                                 | 8       |
| `--qe-bits-accum`        | N/A       | # of bits for quantization of the accumulator                                         | 32      |
| `--qe-clip-acts`         | `--qeca`  | Set activations clipping mode. Choices: "none", "avg", "n_std"                        | "none"  |
| `--qe-clip-n-stds`       | N/A       | When qe-clip-acts is set to 'n_std', this is the number of standard deviations to use | None    |
| `--qe-no-clip-layers`    | `--qencl` | List of layer names (space-separated) for which not to clip activations               |       |
| `--qe-per-channel`       | `--qepc`  | Enable per-channel quantization of weights (per output channel)                       | Off     |
| `--qe-scale-approx-bits` | `--qesab` | Enables scale factor approximation using integer multiply + bit shift, using this number of bits the integer multiplier | None |
| `--qe-stats-file`        | N/A       | Use stats file for static quantization of activations. See details below              | None    |
| `--qe-config-file`       | N/A       | Path to YAML config file. See details below (ignores all other --qe* arguments)       | None    |  

(Note that these arguments can be added to any `argparse.ArgumentParser` by calling `distiller.quantization.add_post_train_quant_args()` and passing an existing parser)

### Command Line vs. YAML Configuration

Post-training quantization can either be configured straight from the command-line, or using a YAML configuration file. Using a YAML file allows fine-grained control over the quantization parameters of each layer, whereas command-line configuration is "monolithic".   
Use the `--qe-config-file` argument to apply a YAML configuration file. See `resnet18_imagenet_post_train.yaml` in this directory for an example of YAML configuration.  
All the examples shown below are using command-line configuration.  

### Static vs. Dynamic Quantization of Activations

Distiller supports both "static" and "dynamic" post-training quantization of **activations**.

* **Static Quantization:** Pre-calculated tensor statistics are used to calculate the quantization parameters. These stats can be generated by using the `--qe-calibration` command-line argument. See `stats/resnet18_quant_stats.yaml` under this directory for more details.
* **Dynamic Quantization:** Quantization parameters are re-calculated for each batch. Support for this mode is **limited** - only convolution, FC (aka Linear) and embedding layers are supported at this time. Non-supported layers are kept in FP32, and a warning is displayed.

All the examples below are using dynamic quantization of activations. For examples using static quantization with a stats file, see the `resnet18_imagenet_post_train.yaml` file mentioned above.  

## Sample Invocations

This table summarizes the settings and results for each run. The command lines for all runs follow in the next table.

|   | Mode       | # Bits Acts | # Bits Weights | Per-Channel | Clip Acts             | Top-1 Accuracy |
|---|------------|-------------|----------------|-------------|-----------------------|----------------|
| 1 | FP32       | 32          | 32             | N/A         |                       | 76.13%         |
| 2 | Symmetric  | 8           | 8              | No          | none                  | 75.42%         |
| 3 | Symmetric  | 8           | 8              | Yes         | none                  | 75.66%         |
| 4 | Symmetric  | 8           | 8              | Yes         | avg                   | 72.54% (See Note 1 below) |
| 5 | Symmetric  | 8           | 8              | Yes         | avg (exc. last layer) | 75.94%         |
| 6 | Asymmetric | 8           | 8              | No          | none                  | 75.90%         |
| 7 | Symmetric  | 6           | 6              | No          | none                  | 48.46% (See Note 2 below) |
| 8 | Asymmetric | 6           | 6              | No          | none                  | 63.31%         |
| 9 | Asymmetric | 6           | 6              | Yes         | avg (exc. last layer) | 73.08%         |

Command lines:

|   | Command Line |
|---|--------------|
| 1 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate`
| 2 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval`
| 3 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-per-channel`
| 4 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-per-channel --qe-clip-acts avg`
| 5 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-per-channel --qe-clip-acts avg --qe-no-clip-layers fc`
| 6 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-mode asym_u`
| 7 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-bits-acts 6 --qe-bits-wts 6`
| 8 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-bits-acts 6 --qe-bits-wts 6 --qe-mode asym_u`
| 9 | `python compress_classifier.py -a resnet50 --pretrained ~/datasets/imagenet --evaluate --quantize-eval --qe-bits-acts 6 --qe-bits-wts 6 --qe-mode asym_u --qe-per-channel --qe-clip-acts avg --qe-no-clip-layers fc`

## Note 1: Accuracy Loss When Clipping Activations

Notice the degradation in accuracy in run (4) - ~3% compared to per-channel without clipping. Let's recall that the output of the final layer of the model holds the "score" of each class (which, since we're using softmax, can be interpreted as the un-normalized log probability of each class). So if we clip the outputs of this layer, we're in fact "cutting-off" the highest (and lowest) scores. If the highest scores for some sample are close enough, this can result in a wrong classification of that sample.  
We can provide Distiller with a list of layers for which not to clip activations. In this case we just want to skip the last layer, which in the case of the ResNet-50 model is called `fc`. This is what we do in run (5), and we regain most of the accuracy back.

## Note 2: Under 8-bits

Runs (7) - (9) are examples of trying post-training quantization below 8-bits. Notice how with the most basic settings we get a massive accuracy loss of almost 28%. Even with asymmetric quantization and all other optimizations enabled, we still get a non-trivial degradation of just over 3% vs. FP32. Quantizing with less than 8-bits, in most cases, required quantization-aware training.