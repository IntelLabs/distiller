#
# Sample configuration file for post-training quantization of ResNet-18.
#
# This allows for more fine-grained control over quantization parameters compared to configuring post-training
# quantization using command-line arguments only (see 'command_line.md' in this directory for examples of that
# method).
#
# The syntax for the YAML configuration file is identical to the scheduler YAML syntax used for configuring
# quantization-aware training / pruning and regularization. The difference is that for post-training quantization we
# only need to define the quantizer itself, without a policy or any other components (e.g. a learning-rate scheduler).
#
# To invoke, run:
# TODO - write the cmdline
#
# Specifically, configuring with a YAML file allows us to define the 'overrides' section, which is cumbersome
# to define programmatically and not exposed as a command-line argument.
#
# To illustrate how this may come in handy, we'll try post-training quantization of ResNet-18 using 6-bits for weights
# and activations. First we'll see what we get when we quantize all layers with 6-bits, and then we'll see how we
# can get better results by selectively quantizing some layers to 8-bits.
#
# We can see that the largest boost to accuracy, ~5%, is obtained by disabling activations clipping for the final layer
# Quantizing the first and last layers to 8 bits instead of 6 bits boosts accuracy by another ~1.1%
# Quantizing the element-wise add layers to 8-bits gives another small boost of ~0.2%

quantizers:
  post_train_quantizer:
    class: PostTrainLinearQuantizer
    bits_activations: 8
    bits_parameters: 8
    bits_accum: 32
    mode: ASYMMETRIC_UNSIGNED
    # Path to stats file assuming this is being invoked from the 'classifier_compression' example directory
    model_activation_stats: ./maskrcnn_quant_stats.yaml
    per_channel_wts: True
    clip_acts: AVG

    # Overrides section for run 3
#    overrides:
#      fc:
#        clip_acts: NONE  # Don't clip activations in last layer before softmax

    # Overrides section for run 4
#    overrides:
#      .*add:
#        bits_weights: 8
#        bits_activations: 8
#      fc:
#        clip_acts: NONE  # Don't clip activations in last layer before softmax

    # Overrides section for run 5
#    overrides:
#    # First and last layers in 8-bits
#      conv1:
#        bits_weights: 8
#        bits_activations: 8
#      fc:
#        bits_weights: 8
#        bits_activations: 8
#        clip_acts: NONE  # Don't clip activations in last layer before softmax

    # Overrides section for run 6
#    overrides:
#    # First and last layers + element-wise add layers in 8-bits
#      conv1:
#        bits_weights: 8
#        bits_activations: 8
#      .*add:
#        bits_weights: 8
#        bits_activations: 8
#      fc:
#        bits_weights: 8
#        bits_activations: 8
#        clip_acts: NONE
